{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab/ Audio/ Audio Summary Generation\n",
    "\n",
    "Auteur: Geoffroy Peeters \n",
    "\n",
    "Last update: 2021/11/24\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this lab is to create a system that allows the automatic generation of an audio summary of a music track from the analysis of its adio content.\n",
    "\n",
    "- the **input** of the system is an audio file (.wav) of a music track\n",
    "- the **output** of the system is another audio file (.wav) that summarizes the content of the input in ``summary_duration_sec`` seconds.\n",
    "\n",
    "The method we will use is the \"summary score\" proposed by [Cooper and Foote, ISMIR 2002].\n",
    "The difference with the original paper (which uses MFCCs as audio features) is the use of Chroma/PCP as audio features.\n",
    "\n",
    "You goal will be to fill in the part between ### START CODE HERE ### and ### END CODE HERE ### to complete the system.\n",
    "\n",
    "The system is splitted into the following steps:\n",
    "\n",
    "- 1) read an audio file\n",
    "- 2) compute observations from the audio file at each time (a.k.a. audio features or hand-crafted features). We will use the Chroma/PCP as audio features.\n",
    "    - 2a) we first need to compute the Short-Time-Fourier-Transform (spectrogram)\n",
    "    - 2b) create a set of filters that allows to map the STFT to the Chroma/PCP space\n",
    "    - 2c) use these filters to map the STFT to the Chroma/PCP space\n",
    "- 3) compute the Self-Similarity-Matrix of the Chroma-gram/PCP-gram\n",
    "- 4) compute the \"summary score\" of the SSM and chooses its maximum value as starting point of the audio summary\n",
    "- 5) write an audio file\n",
    "\n",
    "\n",
    "We will apply the method to the track \"Shivers\" by Ed Sheeran \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/b/b0/Ed_Sheeran_-_Shivers.png\"  width=200 height=200 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "We first import the set of necessay packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "from scipy.signal import convolve2d\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tpt_tools\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Latex\n",
    "do_student = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioFile, start_sec, stop_sec, do_post_processing = './_audio/audio_gammepno.wav', 0, 13, False\n",
    "#audioFile, start_sec, stop_sec, do_post_processing = './_audio/audio_ed_22m.wav', 8, 3*60+47, True\n",
    "\n",
    "sr_hz, audio_v = scipy.io.wavfile.read(audioFile)\n",
    "audio_v = audio_v[start_sec*sr_hz:stop_sec*sr_hz]\n",
    "print('sr_hz is {}'.format(sr_hz))\n",
    "print('size of audio_v is {}'.format(audio_v.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Chroma-gram/PCP-gram from the audio signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STFT window duration\n",
    "L_sec = 0.2\n",
    "# --- STFT hop-size\n",
    "STEP_sec = 0.2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Short Time Fourier Transform\n",
    "\n",
    "We first perform the short term analysis using the succession over time of short term DFT  (a.k.a. Short Time Fourier Transform).\n",
    "We do this using a blackman analysis window of duration 0.2s and hop size 0.2/3s.\n",
    "\n",
    "The conversion from seconds to samples is\n",
    "- ``L_n=round(0.2 * sr_hz)`` et\n",
    "- ``STEP_n=round(0.2/3 * sr_hz)``\n",
    "\n",
    "in which sr_hz is the sampling rate (which depends on the audio file).\n",
    "\n",
    "The DFT size $N_{fft}$ should be larger or equal to the window duration $L_n$. \n",
    "It should be a power of 2 in order to be able to use the Fast Fourier Transform algorithm (FFT). \n",
    "We will use a zero-padding factor of 4 for $N_{fft}$ which means we will take 4 times the value of $N_{fft}$ which is the first power 2 larger than $L_n$.\n",
    "This zero-padding will help us creating the chroma filters for the lowest frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert values from seconds (_sec) to samples (_n)\n",
    "L_n = np.int(np.round(L_sec * sr_hz))\n",
    "STEP_n = int(np.round(STEP_sec*sr_hz))\n",
    "# --- Compute the DFT size using zero-padding\n",
    "Nfft = 4*tpt_tools.nextpow2(L_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each frame, we \n",
    "- window the signal\n",
    "- compute its DFT\n",
    "- store the results in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_getSTFTFromAudio(audio_v, sr_hz, L_n, STEP_n, Nfft):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - audio_v: audio signal\n",
    "        - sr_hz: sampling rate\n",
    "        - L_n: window duration\n",
    "        - STEP_n: hope size\n",
    "        - Nfft: fft size\n",
    "    outputs:\n",
    "        - STFT_ampl_m (Nfft/2+1, nbFrame)\n",
    "        - STFT_freq_hz_v (Nfft/2+1, )\n",
    "        - STFT_time_sec_v (nbFrame, )\n",
    "    \"\"\"\n",
    "    \n",
    "    if do_student:\n",
    "        ### START CODE HERE ###\n",
    "        ...\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    tpt_tools.F_plot2(np.sqrt(STFT_ampl_m), STFT_freq_hz_v, STFT_time_sec_v, 'Frequency [Hz]', 'Time [sec]')\n",
    "    plt.ylim((0,2000));\n",
    "    # +++++++++++++++++++++++++++++++\n",
    "    \n",
    "    return STFT_ampl_m, STFT_freq_hz_v, STFT_time_sec_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STFT_ampl_m, STFT_freq_hz_v, STFT_time_sec_v = F_getSTFTFromAudio(audio_v, sr_hz, L_n, STEP_n, Nfft)\n",
    "\n",
    "print(Nfft)\n",
    "print(STFT_ampl_m.shape)\n",
    "print(STFT_freq_hz_v.shape)\n",
    "print(STFT_time_sec_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function\n",
    "\n",
    "When ```audioFile = './_audio/audio_gammepno.wav'```\n",
    "\n",
    "you should obtain the following values\n",
    "\n",
    "```(65536)\n",
    "(32769, 192)\n",
    "(32769,)\n",
    "(192,)```\n",
    "\n",
    "you should obtain the following figure\n",
    "<img src=\"./_images/TPstruct_01.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the matrix containing the Chroma/PCP filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 12 Chromas (or PCP-Pitch Class Profile). \n",
    "\n",
    "For each chroma ```c```, we will create a specific filter that perform the mapping between the values of the DFT and this specific chroma.\n",
    "\n",
    "We denote by ```n```, the semi-tone-pitches (or midi notes). For example ```n=69``` is A-4 or 440Hz.\n",
    "The chroma ```c``` corresponding to a given semi-note ```n``` is given by ```c = mod(n, 12)```.\n",
    "The chroma ```c``` should therefore represents the energy existing in the DFT at all semi-pitches ```n``` such that ```c = mod(n, 12)```.\n",
    "\n",
    "For each of these ```n```, we will create a band-pass filter centered on $n$ and extending on the left to ```n-1``` and the right to ```n+1```.\n",
    "\n",
    "Each band-pass filter is normalized such that it sums to 1.\n",
    "\n",
    "To create the filters we can use the following formula which maps the DFT frequencies (in Hz) ```STFT_freq_hz_v```to the corresponding midi notes ```STFT_freq_midi_v```: ```STFT_freq_midi_v = 12 * log2(STFT_freq_hz_v / 440)+69```.\n",
    "\n",
    "The shape of the filter will be defined by the following formula:\n",
    "$$H = \\frac{1}{2} \\tanh(\\pi (1-2x))+\\frac{1}{2}$$ \n",
    "in which $x$ represents the absolute value of the distance between ```n``` and ```STFT_freq_midi_v```: ```x = abs(n - STFT_freq_midi_v)```.\n",
    "\n",
    "We will only consider the MIDI notes from 36 to 119: ```n=36 ... 119```.\n",
    "\n",
    "The 12 chroma filters are stored in a matrix ```(12, Nfft/2+1)```, i.e. one filter per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_getChromaFiltre(STFT_freq_hz_v, startMidi=36, endMidi=119):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - STFT_freq_hz_v (N/2+1): frequency axis of the DFT in Hz\n",
    "        - startMidi: starting midi note (int)\n",
    "        - endMidi: ending midi note (int)\n",
    "    outputs:\n",
    "        - chromaFiltre_m (12, N/2+1)\n",
    "        - midi_v (nbMidi)\n",
    "    \"\"\"\n",
    "    \n",
    "    if do_student:\n",
    "        ### START CODE HERE ###\n",
    "        ...\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # +++++++++++++++++++++++++++++++\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    tpt_tools.F_plot1(STFT_freq_hz_v, chromaFiltre_m[8,:], 'Frequency [Hz]', 'Value')\n",
    "    plt.xlim((0,2000));\n",
    "    # +++++++++++++++++++++++++++++++\n",
    "    \n",
    "    return chromaFiltre_m, midi_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromaFiltre_m, midi_v = F_getChromaFiltre(STFT_freq_hz_v)\n",
    "\n",
    "print(chromaFiltre_m.shape)\n",
    "print(midi_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function\n",
    "\n",
    " You should obtain the following values\n",
    " \n",
    " ``(12, 32769)\n",
    "(84,)``\n",
    "\n",
    "You should obtain the following figure\n",
    "<img src=\"./_images/TPstruct_02.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display all the filters as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "tpt_tools.F_plot2(chromaFiltre_m, np.arange(0, 12), STFT_freq_hz_v, 'Chroma', 'Frequency [Hz]')\n",
    "plt.xlim((0,1000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should obtain the following figure\n",
    "<img src=\"./_images/TPstruct_03.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Chromagram\n",
    "\n",
    "To obtain the Chroma-gram/PCP-gram, we simply multiply (matrix multiplication) the Chroma/PCP matrix with the STFT matrix:\n",
    "(12, N/2+1) * (N/2+1, nbFrame) = (12, nbFrame). \n",
    "\n",
    "The output of this is a matrix in which the rows represent the 12 values of the chroma and the column the successive time frames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chromaGram_m = np.dot(chromaFiltre_m, STFT_ampl_m)\n",
    "\n",
    "print(chromaFiltre_m.shape)\n",
    "print(STFT_ampl_m.shape)\n",
    "print(chromaGram_m.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "tpt_tools.F_plot2(chromaGram_m, np.arange(0, 12), STFT_time_sec_v, 'Chroma', 'Time [sec]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When ```audioFile = './_audio/audio_gammepno.wav'```, \n",
    "\n",
    "you should obtain the following values\n",
    "\n",
    "```(12, 32769)\n",
    "(32769, 192)\n",
    "(12, 192)```\n",
    "\n",
    "you should obtain the following figure\n",
    "<img src=\"./_images/TPstruct_04.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "To make the visualisation/detection of the structure easier we apply some post-processing. We first smooth the chromagram over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_post_processing:\n",
    "    L = 21\n",
    "    chromaGramSmooth_m = np.zeros(chromaGram_m.shape)\n",
    "    for dim in range(12):\n",
    "        chromaGramSmooth_m[dim,:] = scipy.signal.lfilter(np.ones(L)/L, 1, chromaGram_m[dim,:])\n",
    "    chromaGram_m = chromaGramSmooth_m\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    tpt_tools.F_plot2(chromaGram_m, np.arange(0, 12), STFT_time_sec_v, 'Chroma', 'Time [sec]')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the Self-Similarity-Matrix (time,time) using of the Chroma-gram/PCP-gram\n",
    "\n",
    "We now compute the Self-Similarity-Matrix (SSM) using the Chroma-gram/PCP-gram.\n",
    "An entry $(i,j)$ in the SSM represent the similarity between time $ t_i $ and time $t_j$.\n",
    "\n",
    "To compute this similarity, we will use the ```cosine distance``` between the chroma vector at time $t_i$ and the one at $t_j$.\n",
    "\n",
    "If we note $\\underline{x}$ the vector at time $t_i$ and $\\underline{y}$ the vector at time $t_j$, the ```cosine distance``` between them is defined as \n",
    "\n",
    "$$dist(\\underline{x},\\underline{y})=\\frac{\\underline{x} \\cdot \\underline{y}}{||\\underline{x}|| \\;||\\underline{y}||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_getSsmTT(data_m):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - data_m (nbDim, nbFrame)\n",
    "    outputs:\n",
    "        - ssmTT_m (nbFrame, nbFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    if do_student:\n",
    "        ### START CODE HERE ###\n",
    "        ...\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return ssmTT_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssmTT_m = F_getSsmTT(chromaGram_m)\n",
    "\n",
    "# --- Display the matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "tpt_tools.F_plot2(ssmTT_m, STFT_time_sec_v, STFT_time_sec_v, 'Time [sec]', 'Time [sec]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function\n",
    "\n",
    "When ```audioFile = './_audio/audio_gammepno.wav'```, \n",
    "\n",
    "you should obtain the following figure\n",
    "\n",
    "<img src=\"./_images/TPstruct_05.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "To make the visualisation/detection of the structure easier we apply some post-processing. \n",
    "We apply a strong threshold to the SSM to only let the high values visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_post_processing:\n",
    "    ssmTT_m[ssmTT_m<0.98]=0\n",
    "\n",
    "    # --- Display the matrix\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    tpt_tools.F_plot2(ssmTT_m, STFT_time_sec_v, STFT_time_sec_v, 'Time [sec]', 'Time [sec]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the best position for the start of the summary\n",
    "\n",
    "To get the best position for the start of the summary ``startPos_frame``, we will compute the ``summary_score_v`` of the SSM. \n",
    "\n",
    "We then choose its maximum value as the start of our audio summary.\n",
    "Indeed, this maximum value represent the best starting point of a segment of duration ``summary_duration_sec`` such that this segment best represent (is the most similar to) the whole content of the music track.\n",
    "\n",
    "Computation of the ``summary_score_v`` as proposed by [Cooper, Foote, 2002]:\n",
    "- For each time ``q`` (row q in the matrix), the summary score is computed as the sum over the column of the matrix for this specific row. It represents the similarity between the specific time ``q`` and the rest of the music track.\n",
    "- For a segment starting at ``q`` and of duration ``L``, the summary score is computed as the sum over the part of the columns between ``q`` and ``q+L``. It represents the similarity between the segment ``[q,q+L]`` and the rest of the music track.\n",
    "- ``L`` is imposed as the length of the audio summary (``summary_duration_sec`` in our case).\n",
    "\n",
    "Since we look for the best summary of duration ``summary_duration_sec``, we look for the value of ``q`` which maximizes the summary score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_getSummaryScore(ssmTT_m, summary_duration_frame):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - ssmTT_m (nbFrame, nbFrame)\n",
    "        - summary_duration_frame\n",
    "    outputs:\n",
    "        - startPos_frame\n",
    "        - summary_score_v (nbFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    if do_student:\n",
    "        ### START CODE HERE ###\n",
    "        ...\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return startPos_frame, summary_score_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the summary length for various duration of summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define the duration of the summary in seconds\n",
    "summary_duration_sec = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- convert it to number of frames\n",
    "summary_duration_frame, summary_duration_n = int(summary_duration_sec/STEP_sec), int(summary_duration_sec*sr_hz)\n",
    "print(\"Duration in sec:{} in samples:{} in frames:{}\".format(summary_duration_sec, summary_duration_n, summary_duration_frame))\n",
    "\n",
    "startPos_frame, summary_score_v = F_getSummaryScore(ssmTT_m, summary_duration_frame)\n",
    "startPos_sec, startPos_n = startPos_frame*STEP_sec, int(startPos_frame*STEP_sec*sr_hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the resuting score function and estimated starting point of the audio summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "tpt_tools.F_plot1(STFT_time_sec_v, summary_score_v, 'frame', 'score')\n",
    "plt.plot(startPos_sec, summary_score_v[startPos_frame], 'ro')\n",
    "print(\"Start sec:{}, samples:{} frames:{}\".format(startPos_sec, startPos_n, startPos_frame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the selected segment over the Self-Similarity-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "tpt_tools.F_plot2(ssmTT_m, STFT_time_sec_v, STFT_time_sec_v, 'Time [sec]', 'Time [sec]')\n",
    "plt.plot(startPos_sec, startPos_sec, 'ro')\n",
    "plt.plot(startPos_sec+summary_duration_sec, startPos_sec+summary_duration_sec, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the audio summary\n",
    "\n",
    "The audio summary is then simply the segment starting at ```startPos``` and of duration ```duration```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.wavfile.write(audioFile.replace('.wav', '-summary.wav'), sr_hz, audio_v[startPos_n:startPos_n+summary_duration_n])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "176.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
